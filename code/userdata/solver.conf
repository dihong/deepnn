algorithm: SGD	  # stochastic gradient descent (SGD), adaptive gradient (ADAGRAD), Nesterovâ€™s accelerated gradient (NESTEROV). 

base_lr: 0.01     # initial learning rate.

gamma: 0.1        # In every 'stepsize' iterations, drop the learning rate by a factor of 10 (i.e., multiply it by a factor of gamma = 0.1) 

stepsize: 100000  # drop the learning rate every 'stepsize' iterations

max_iter: 350000  # train for 350K iterations total

momentum: 0.9	  # (not valid for ADAGRAD solver).
